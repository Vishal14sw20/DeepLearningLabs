{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Copy of week1_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zJDBuo88EuEV",
        "kWQpaTwcEuEW",
        "Yq41x1WXEuEX",
        "mrByOQ7SEuEY",
        "wW0H2NFuEuFd",
        "sjbCt-ECEuF8"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal14sw20/DeepLearningLabs/blob/master/week1_ex_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49LMF2STEuBE",
        "colab_type": "text"
      },
      "source": [
        "# Data Manipulation\n",
        "\n",
        "It is impossible to get anything done if we cannot manipulate data. Generally, there are two important things we need to do with data: (i) acquire it and (ii) process it once it is inside the computer. There is no point in acquiring data if we do not even know how to store it, so let's get our hands dirty first by playing with synthetic data. We will start by introducing the tensor,\n",
        "PyTorch's primary tool for storing and transforming data. If you have worked with NumPy before, you will notice that tensors are, by design, similar to NumPy's multi-dimensional array. Tensors support asynchronous computation on CPU, GPU and provide support for automatic differentiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbbSn12iEuBG",
        "colab_type": "text"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MML50_vEuBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMZyDwCmEuBM",
        "colab_type": "text"
      },
      "source": [
        "Tensors represent (possibly multi-dimensional) arrays of numerical values.\n",
        "The simplest object we can create is a vector. To start, we can use `arange` to create a row vector with 12 consecutive integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Qm6OdfEuBM",
        "colab_type": "code",
        "outputId": "5dfa3361-00bd-4eb5-8509-f9239996ae63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = torch.arange(12, dtype=torch.float64)\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOEXXRO8EuBR",
        "colab_type": "code",
        "outputId": "0b14d6f3-0cf2-4b3e-b90f-de47c7a177df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We can get the tensor shape through the shape attribute.\n",
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RIh_-uQ3EuBV",
        "colab_type": "code",
        "outputId": "20533158-6bc2-40fa-8125-c7529e19b0fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# .shape is an alias for .size(), and was added to more closely match numpy\n",
        "x.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMt9PPSoEuBb",
        "colab_type": "text"
      },
      "source": [
        "We use the `reshape` function to change the shape of one (possibly multi-dimensional) array, to another that contains the same number of elements.\n",
        "For example, we can transform the shape of our line vector `x` to (3, 4), which contains the same values but interprets them as a matrix containing 3 rows and 4 columns. Note that although the shape has changed, the elements in `x` have not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VaNDjNqEuBc",
        "colab_type": "code",
        "outputId": "f8d76d85-b438-4459-a5ed-865c5878aeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x = x.reshape((3, 4))\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnUFcot0EuBf",
        "colab_type": "text"
      },
      "source": [
        "Reshaping by manually specifying each of the dimensions can get annoying. Once we know one of the dimensions, why should we have to perform the division our selves to determine the other? For example, above, to get a matrix with 3 rows, we had to specify that it should have 4 columns (to account for the 12 elements). Fortunately, PyTorch can automatically work out one dimension given the other.\n",
        "We can invoke this capability by placing `-1` for the dimension that we would like PyTorch to automatically infer. In our case, instead of\n",
        "`x.reshape((3, 4))`, we could have equivalently used `x.reshape((-1, 4))` or `x.reshape((3, -1))`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30ra6ku4EuBg",
        "colab_type": "code",
        "outputId": "4b5d9123-0907-4892-c138-7d994305a135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.FloatTensor(2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.1848e-36, 0.0000e+00, 3.7835e-44],\n",
              "        [0.0000e+00,        nan, 0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsQ1d5CtEuBk",
        "colab_type": "code",
        "outputId": "df0c7305-4f5a-48ee-e060-58b6bd3aa9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.Tensor(2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.1848e-36, 0.0000e+00, 8.4078e-45],\n",
              "        [0.0000e+00, 1.4013e-45, 0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoyNna8BEuBo",
        "colab_type": "code",
        "outputId": "d18324a6-aef7-46d6-8a24-dfb7077fc3af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.empty(2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[9.1848e-36, 0.0000e+00, 3.3631e-44],\n",
              "        [0.0000e+00,        nan, 0.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g2Uiu1EEuBq",
        "colab_type": "text"
      },
      "source": [
        "torch.Tensor() is just an alias to torch.FloatTensor() which is the default type of tensor, when no dtype is specified during tensor construction.\n",
        "\n",
        "From the torch for numpy users notes, it seems that torch.Tensor() is a drop-in replacement of numpy.empty()\n",
        "\n",
        "So, in essence torch.FloatTensor() and torch.empty() does the same job.\n",
        "\n",
        "The `empty` method just grabs some memory and hands us back a matrix without setting the values of any of its entries. This is very efficient but it means that the entries might take any arbitrary values, including very big ones! Typically, we'll want our matrices initialized either with ones, zeros, some known constant or numbers randomly sampled from a known distribution.\n",
        "\n",
        "Perhaps most often, we want an array of all zeros. To create tensor with all elements set to 0 and a shape of (2, 3, 4) we can invoke:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUNIY4-UEuBr",
        "colab_type": "code",
        "outputId": "37eefc6b-0f3f-40c3-c418-902f0c3233da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.zeros((2, 3, 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYNE3c6lEuBu",
        "colab_type": "text"
      },
      "source": [
        "We can create tensors with each element set to 1 works via"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0I4FOgbEuBv",
        "colab_type": "code",
        "outputId": "d743cba2-8c50-4522-8206-8457d48f8b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.ones((2, 3 , 4 ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Do5YcvDEuBz",
        "colab_type": "text"
      },
      "source": [
        "We can also specify the value of each element in the desired NDArray by supplying a Python list containing the numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRRAj9HZEuBz",
        "colab_type": "code",
        "outputId": "afa3d989-bc77-431c-8604-1e3be76e57ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVS0z7mOEuB2",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** In some cases, we will want to randomly sample the values of each element in the tensor according to some known probability distribution. This is especially common when we intend to use the tensor as a parameter in a neural network. The following function should create a tensor with a shape of (3,4). Each of its elements is randomly sampled in a normal distribution with zero mean and unit variance. Fill out the following function using torch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM45Wwm0EuB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rand_matrix(rows=3, cols=4):\n",
        "  ## write your code below\n",
        "  torch.empty(rows,cols).normal_(0,1)\n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K5SKilxEuB-",
        "colab_type": "text"
      },
      "source": [
        "## Operations\n",
        "\n",
        "Oftentimes, we want to apply functions to arrays. Some of the simplest and most useful functions are the element-wise functions. These operate by performing a single scalar operation on the corresponding elements of two arrays. We can create an element-wise function from any function that maps from the scalars to the scalars. In math notations we would denote such a function as $f: \\mathbb{R} \\rightarrow \\mathbb{R}$. Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*, and the function f,\n",
        "we can produce a vector $\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$ by setting $c_i \\gets f(u_i, v_i)$ for all $i$. Here, we produced the vector-valued $F: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ by *lifting* the scalar function to an element-wise vector operation. In PyTorch, the common standard arithmetic operators (+,-,/,\\*,\\*\\*) have all been *lifted* to element-wise operations for identically-shaped tensors of arbitrary shape. We can call element-wise operations on any two tensors of the same shape, including matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VodPvAwREuB-",
        "colab_type": "code",
        "outputId": "166cb529-9587-439d-c15b-51c260b779d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "x = torch.tensor([1, 2, 4, 8], dtype=torch.float32)\n",
        "y = torch.ones_like(x) * 2\n",
        "print('x =', x)\n",
        "print('y =', y)\n",
        "print('x + y', x + y)\n",
        "print('x - y', x - y)\n",
        "print('x * y', x * y)\n",
        "print('x / y', x / y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x = tensor([1., 2., 4., 8.])\n",
            "y = tensor([2., 2., 2., 2.])\n",
            "x + y tensor([ 3.,  4.,  6., 10.])\n",
            "x - y tensor([-1.,  0.,  2.,  6.])\n",
            "x * y tensor([ 2.,  4.,  8., 16.])\n",
            "x / y tensor([0.5000, 1.0000, 2.0000, 4.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwcD7wcCEuCB",
        "colab_type": "text"
      },
      "source": [
        "Many more operations can be applied element-wise, such as exponentiation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9WWVuyNEuCC",
        "colab_type": "code",
        "outputId": "3c630513-06ba-4507-b5a2-e64ae261c197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.exp(x)\n",
        "# Note: torch.exp is not implemented for 'torch.LongTensor'."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xDYKKI9EuCF",
        "colab_type": "text"
      },
      "source": [
        "In addition to computations by element, we can also perform matrix operations, like matrix multiplication using the `mm` or `matmul` function. Next, we will perform matrix multiplication of `x` and the transpose of `y`. We define `x` as a matrix of 3 rows and 4 columns, and `y` is transposed into a matrix of 4 rows and 3 columns. The two matrices are multiplied to obtain a matrix of 3 rows and 3 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECvodY28EuCG",
        "colab_type": "code",
        "outputId": "1e4a4c36-ad84-4bfd-808b-5106c6c6ba8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
        "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
        "print(x.dtype)\n",
        "print(y)\n",
        "torch.mm(x, y.t())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "tensor([[2., 1., 4., 3.],\n",
            "        [1., 2., 3., 4.],\n",
            "        [4., 3., 2., 1.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 18.,  20.,  10.],\n",
              "        [ 58.,  60.,  50.],\n",
              "        [ 98., 100.,  90.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JYQVpxJEuCK",
        "colab_type": "text"
      },
      "source": [
        "Note that torch.dot() behaves differently to np.dot(). There's been some discussion about what would be desirable here. Specifically, torch.dot() treats both a and b as 1D vectors (irrespective of their original shape) and computes their inner product. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0TtNQf-EuCK",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** We can also merge multiple tensors. For that, we need to tell torch along which dimension to merge. For instance, we can merge two matrices (tensors) along dimension 0 (along rows) and dimension 1 (along columns). Write a function that does that using torch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJEQ2N5WEuCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_tensors(x, y, dim):\n",
        "  ## write your code below\n",
        "  \n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIFHNal_EuCR",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, we may want to construct binary tensors via logical statements. Take `x == y` as an example. If `x` and `y` are equal for some entry, the new tensor has a value of 1 at the same position; otherwise it is 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNJPemjEuCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x == y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2x7MjazEuCU",
        "colab_type": "text"
      },
      "source": [
        "Summing all the elements in the tensor yields an tensor with only one element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxwYNhTmEuCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epYhypmSEuCY",
        "colab_type": "text"
      },
      "source": [
        "We can transform the result into a scalar in Python using the `asscalar` function of `numpy`. In the following example, the $\\ell_2$ norm of `x` yields a single element tensor. The final result is transformed into a scalar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lIokP5EEuCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.asscalar(x.norm())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLgRF4q_EuCb",
        "colab_type": "text"
      },
      "source": [
        "## Broadcast Mechanism\n",
        "\n",
        "In the above section, we saw how to perform operations on two tensors of the same shape. When their shapes differ, a broadcasting mechanism may be triggered analogous to NumPy: first, copy the elements appropriately so that the two tensors have the same shape, and then carry out operations by element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AapYKFgNEuCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.arange(3, dtype=torch.float).reshape((3, 1))\n",
        "b = torch.arange(2, dtype=torch.float).reshape((1, 2))\n",
        "a, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQTw96hCEuCe",
        "colab_type": "text"
      },
      "source": [
        "Since `a` and `b` are (3x1) and (1x2) matrices respectively, their shapes do not match up if we want to add them. PyTorch addresses this by 'broadcasting' the entries of both matrices into a larger (3x2) matrix as follows: for matrix `a` it replicates the columns, for matrix `b` it replicates the rows before adding up both element-wise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oduVCdFSEuCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9uhAfDEuCi",
        "colab_type": "text"
      },
      "source": [
        "## Indexing and Slicing\n",
        "\n",
        "Just like in any other Python array, elements in a tensor can be accessed by its index. In good Python tradition the first element has index 0 and ranges are specified to include the first but not the last element. By this logic `1:3` selects the second and third element. Let's try this out by selecting the respective rows in a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulp_pGziEuCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x[1:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xw3TEL_EuCl",
        "colab_type": "text"
      },
      "source": [
        "Beyond reading, we can also write elements of a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgxSajDpEuCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x[1, 2] = 9\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUKIPJKWEuCo",
        "colab_type": "text"
      },
      "source": [
        "If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, `[0:2, :]` accesses the first and second rows. While we discussed indexing for matrices, this obviously also works for vectors and for tensors of more than 2 dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSY-aW2PEuCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x[0:2, :] = 12\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0h5EaBmEuCr",
        "colab_type": "text"
      },
      "source": [
        "## Saving Memory\n",
        "\n",
        "In the previous example, every time we ran an operation, we allocated new memory to host its results. For example, if we write `y = x + y`, we will dereference the matrix that `y` used to point to and instead point it at the newly allocated memory. In the following example we demonstrate this with Python's `id()` function, which gives us the exact address of the referenced object in memory. After running `y = y + x`, we will find that `id(y)` points to a different location. That is because Python first evaluates `y + x`, allocating new memory for the result and then subsequently redirects `y` to point at this new location in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVDlqSpEuCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = id(y)\n",
        "y = y + x\n",
        "id(y) == before"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py2TRyNZEuCv",
        "colab_type": "text"
      },
      "source": [
        "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates *in place*. Second, we might point at the same parameters from multiple variables. If we do not update in place, this could cause a memory leak, making it possible for us to inadvertently reference stale parameters.\n",
        "\n",
        "Fortunately, performing in-place operations in PyTorch is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., `y[:] = <expression>`. To illustrate the behavior, we first clone the shape of a matrix using `zeros_like` to allocate a block of 0 entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8wkcVZKEuCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = torch.zeros_like(y)\n",
        "print('id(z):', id(z))\n",
        "z[:] = x + y\n",
        "print('id(z):', id(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP5UeVueEuCz",
        "colab_type": "text"
      },
      "source": [
        "While this looks pretty, `x+y` here will still allocate a temporary buffer to store the result of `x+y` before copying it to `z[:]`. To make even better use of memory, we can directly invoke the underlying `tensor` operation, in this case `add`, avoiding temporary buffers. We do this by specifying the `out` keyword argument, which every `tensor` operator supports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9-_TWe8EuCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = id(z)\n",
        "torch.add(x, y, out=z)\n",
        "id(z) == before"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4hS4UOEuC2",
        "colab_type": "text"
      },
      "source": [
        "If the value of `x ` is not reused in subsequent computations, we can also use `x[:] = x + y` or `x += y` to reduce the memory overhead of the operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGkER3zUEuC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "before = id(x)\n",
        "x += y\n",
        "id(x) == before"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXOkjW3lEuC5",
        "colab_type": "text"
      },
      "source": [
        "## Mutual Transformation of PyTorch and NumPy\n",
        "\n",
        "Converting PyTorch Tensors to and from NumPy Arrays is easy. The converted arrays do *not* share memory. This minor inconvenience is actually quite important: when you perform operations on the CPU or one of the GPUs, you do not want PyTorch having to wait whether NumPy might want to be doing something else with the same chunk of memory. `.tensor` and `.numpy` do the trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpG6NtjoEuC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = x.numpy()\n",
        "print(type(a))\n",
        "b = torch.tensor(a)\n",
        "print(type(b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jPw7YWLEuC-",
        "colab_type": "text"
      },
      "source": [
        "# Linear Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyz1HiEHEuDA",
        "colab_type": "text"
      },
      "source": [
        "Now that you can store and manipulate data,\n",
        "let's briefly review the subset of basic linear algebra\n",
        "that you will need to understand most of the models.\n",
        "We will introduce all the basic concepts,\n",
        "the corresponding mathematical notation,\n",
        "and their realization in code all in one place.\n",
        "If you are already confident in your basic linear algebra,\n",
        "feel free to skim through or skip this chapter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA_59-abEuDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRJr0LrwEuDE",
        "colab_type": "text"
      },
      "source": [
        "## Scalars\n",
        "\n",
        "If you never studied linear algebra or machine learning,\n",
        "you are probably used to working with one number at a time.\n",
        "And know how to do basic things like add them together or multiply them.\n",
        "For example, in Palo Alto, the temperature is $52$ degrees Fahrenheit.\n",
        "Formally, we call these values $scalars$.\n",
        "If you wanted to convert this value to Celsius (using metric system's more sensible unit of temperature measurement),\n",
        "you would evaluate the expression $c = (f - 32) * 5/9$ setting $f$ to $52$.\n",
        "In this equation, each of the terms $32$, $5$, and $9$ is a scalar value.\n",
        "The placeholders $c$ and $f$ that we use are called variables\n",
        "and they represent unknown scalar values.\n",
        "\n",
        "In mathematical notation, we represent scalars with ordinary lower-cased letters ($x$, $y$, $z$).\n",
        "We also denote the space of all scalars as $\\mathcal{R}$.\n",
        "For expedience, we are going to punt a bit on what precisely a space is,\n",
        "but for now, remember that if you want to say that $x$ is a scalar,\n",
        "you can simply say $x \\in \\mathcal{R}$.\n",
        "The symbol $\\in$ can be pronounced \"in\" and just denotes membership in a set.\n",
        "\n",
        "In PyTorch, we work with scalars by creating tensors with just one element.\n",
        "In this snippet, we instantiate two scalars and perform some familiar arithmetic operations with them, such as addition, multiplication, division and exponentiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SosJSDWREuDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor([3.0])\n",
        "y = torch.tensor([2.0])\n",
        "\n",
        "print('x + y = ', x + y)\n",
        "print('x * y = ', x * y)\n",
        "print('x / y = ', x / y)\n",
        "print('x ** y = ', torch.pow(x,y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytT9xv6hEuDI",
        "colab_type": "text"
      },
      "source": [
        "We can convert any tensor to a Python float by calling its `item` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDLbJOEwEuDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRySorLjEuDO",
        "colab_type": "text"
      },
      "source": [
        "## Vectors\n",
        "\n",
        "You can think of a vector as simply a list of numbers, for example ``[1.0,3.0,4.0,2.0]``.\n",
        "Each of the numbers in the vector consists of a single scalar value.\n",
        "We call these values the *entries* or *components* of the vector.\n",
        "Often, we are interested in vectors whose values hold some real-world significance.\n",
        "For example, if we are studying the risk that loans default,\n",
        "we might associate each applicant with a vector\n",
        "whose components correspond to their income,\n",
        "length of employment, number of previous defaults, etc.\n",
        "If we were studying the risk of heart attacks hospital patients potentially face,\n",
        "we might represent each patient with a vector\n",
        "whose components capture their most recent vital signs,\n",
        "cholesterol levels, minutes of exercise per day, etc.\n",
        "In math notation, we will usually denote vectors as bold-faced,\n",
        "lower-cased letters ($\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{w})$.\n",
        "In PyTorch, we work with vectors via 1D tensors with an arbitrary number of components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQZ75s0YEuDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.arange(4)\n",
        "print('x = ', x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8RwAmKFEuDT",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** We can refer to any element of a vector by using a subscript.\n",
        "For example, we can refer to the $4$th element of $\\mathbf{u}$ by $u_4$.\n",
        "Note that the element $u_4$ is a scalar,\n",
        "so we do not bold-face the font when referring to it.\n",
        "In code, we access any element $i$ by indexing into the ``tensor``. Write a function that returns the i-th element from a tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsRfQYkiEuDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_indexing(x, i):\n",
        "  ## write your code below\n",
        "\n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh1VnTYxEuDW",
        "colab_type": "text"
      },
      "source": [
        "## Length, dimensionality and shape\n",
        "\n",
        "Let's revisit some concepts from the previous section. A vector is just an array of numbers. And just as every array has a length, so does every vector.\n",
        "In math notation, if we want to say that a vector $\\mathbf{x}$ consists of $n$ real-valued scalars,\n",
        "we can express this as $\\mathbf{x} \\in \\mathcal{R}^n$.\n",
        "The length of a vector is commonly called its $dimension$.\n",
        "As with an ordinary Python array, we can access the length of a tensor\n",
        "by calling Python's in-built ``len()`` function.\n",
        "\n",
        "We can also access a vector's length via its `.shape` attribute.\n",
        "The shape is a tuple that lists the dimensionality of the tensor along each of its axes.\n",
        "Because a vector can only be indexed along one axis, its shape has just one element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_opEJS1EuDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtcoj1yhEuDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUlZQai7EuDe",
        "colab_type": "text"
      },
      "source": [
        "Note that the word dimension is overloaded and this tends to confuse people.\n",
        "Some use the *dimensionality* of a vector to refer to its length (the number of components).\n",
        "However some use the word *dimensionality* to refer to the number of axes that an array has.\n",
        "In this sense, a scalar *would have* $0$ dimensions and a vector *would have* $1$ dimension.\n",
        "\n",
        "**To avoid confusion, when we say *2D* array or *3D* array, we mean an array with 2 or 3 axes respectively. But if we say *$n$-dimensional* vector, we mean a vector of length $n$.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwsztwrhEuDf",
        "colab_type": "code",
        "outputId": "e4440d42-efa8-4ef0-ea81-9c821a8eda7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = 2\n",
        "x = torch.tensor([1,2,3])\n",
        "y = torch.tensor([10,20,30])\n",
        "print(a * x)\n",
        "print(a * x + y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 4, 6])\n",
            "tensor([12, 24, 36])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agBrm6ZaEuDi",
        "colab_type": "text"
      },
      "source": [
        "## Matrices\n",
        "\n",
        "Just as vectors generalize scalars from order $0$ to order $1$,\n",
        "matrices generalize vectors from $1D$ to $2D$.\n",
        "Matrices, which we'll typically denote with capital letters ($A$, $B$, $C$),\n",
        "are represented in code as tensors with 2 axes.\n",
        "Visually, we can draw a matrix as a table,\n",
        "where each entry $a_{ij}$ belongs to the $i$-th row and $j$-th column.\n",
        "\n",
        "\n",
        "$$A=\\begin{pmatrix}\n",
        " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
        " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "We can create a matrix with $n$ rows and $m$ columns in PyTorch\n",
        "by specifying a shape with two components `(n,m)`\n",
        "when calling any of our favorite functions for instantiating an `tensor`\n",
        "such as `ones`, or `zeros`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvFjYjVzEuDj",
        "colab_type": "code",
        "outputId": "c9cfaa15-dd65-4f70-a62a-c01dabf3c0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape((5,4))\n",
        "print(A)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.],\n",
            "        [12., 13., 14., 15.],\n",
            "        [16., 17., 18., 19.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELSd1P-4EuDl",
        "colab_type": "text"
      },
      "source": [
        "Matrices are useful data structures: they allow us to organize data that has different modalities of variation. For example, rows in our matrix might correspond to different patients, while columns might correspond to different attributes.\n",
        "\n",
        "We can access the scalar elements $a_{ij}$ of a matrix $A$ by specifying the indices for the row ($i$) and column ($j$) respectively. Leaving them blank via a `:` takes all elements along the respective dimension (as seen in the previous section).\n",
        "\n",
        "We can transpose the matrix through `t()`. That is, if $B = A^T$, then $b_{ij} = a_{ji}$ for any $i$ and $j$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoeZDMcXEuDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(A.t())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSjRm8BAEuDo",
        "colab_type": "text"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures with even more axes. Tensors give us a generic way of discussing arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors.\n",
        "\n",
        "Using tensors will become more important when we start working with images, which arrive as 3D data structures, with axes corresponding to the height, width, and the three (RGB) color channels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HG2LJ6NEuDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.arange(24).reshape((2, 3, 4))\n",
        "print('X.shape =', X.shape)\n",
        "print('X =', X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoLZOWfgEuDr",
        "colab_type": "text"
      },
      "source": [
        "## Basic properties of tensor arithmetic\n",
        "\n",
        "Scalars, vectors, matrices, and tensors of any order have some nice properties that we will often rely on.\n",
        "For example, as you might have noticed from the definition of an element-wise operation,\n",
        "given operands with the same shape,\n",
        "the result of any element-wise operation is a tensor of that same shape.\n",
        "Another convenient property is that for all tensors, multiplication by a scalar\n",
        "produces a tensor of the same shape.\n",
        "In math, given two tensors $X$ and $Y$ with the same shape,\n",
        "$\\alpha X + Y$ has the same shape\n",
        "(numerical mathematicians call this the AXPY operation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KgnE63qEuDr",
        "colab_type": "code",
        "outputId": "e0e96c7e-937b-4fff-f04f-dae0c63accc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "a = 2\n",
        "x = torch.ones(3)\n",
        "y = torch.zeros(3)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print((a * x).shape)\n",
        "print((a * x + y).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n",
            "torch.Size([3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKYV-YRcEuDu",
        "colab_type": "text"
      },
      "source": [
        "Shape is not the the only property preserved under addition and multiplication by a scalar. These operations also preserve membership in a vector space. But we will postpone this discussion for the second half of this chapter because it is not critical to getting your first models up and running.\n",
        "\n",
        "## Sums and means\n",
        "\n",
        "The next more sophisticated thing we can do with arbitrary tensors\n",
        "is to calculate the sum of their elements.\n",
        "In mathematical notation, we express sums using the $\\sum$ symbol.\n",
        "To express the sum of the elements in a vector $\\mathbf{u}$ of length $d$,\n",
        "we can write $\\sum_{i=1}^d u_i$. In code, we can just call `torch.sum()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAEW1JBxEuDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x)\n",
        "print(torch.sum(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye5RRF_lEuDy",
        "colab_type": "text"
      },
      "source": [
        "We can similarly express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix $A$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOmNS4HeEuDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(A)\n",
        "print(torch.sum(A))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq_k4NNqEuD2",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** A related quantity is the *mean*, which is also called the *average*.\n",
        "We calculate the mean by dividing the sum by the total number of elements.\n",
        "With mathematical notation, we could write the average\n",
        "over a vector $\\mathbf{u}$ as $\\frac{1}{d} \\sum_{i=1}^{d} u_i$\n",
        "and the average over a matrix $A$ as  $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$. Write a function that computes the mean of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ1404UiEuD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_mean(A):\n",
        "  ## write your code below\n",
        "\n",
        "  ## end of the function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ey5y1IuEuD7",
        "colab_type": "text"
      },
      "source": [
        "## Dot products\n",
        "\n",
        "So far, we have only performed element-wise operations, sums and averages. And if this was all we could do, linear algebra probably would not deserve its own chapter. However, one of the most fundamental operations is the dot product. Given two vectors $\\mathbf{u}$ and $\\mathbf{v}$, the dot product $\\mathbf{u}^T \\mathbf{v}$ is a sum over the products of the corresponding elements: $\\mathbf{u}^T \\mathbf{v} = \\sum_{i=1}^{d} u_i \\cdot v_i$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5Rx_cWzEuD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.arange(4, dtype = torch.float32)\n",
        "y = torch.ones(4, dtype = torch.float32)\n",
        "print(x, y, torch.dot(x, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm8kd05CEuEE",
        "colab_type": "text"
      },
      "source": [
        "Note that we can express the dot product of two vectors ``torch.dot(x, y)`` equivalently by performing an element-wise multiplication and then a sum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5yqDhlgEuEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.sum(x * y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHaJPx2TEuEH",
        "colab_type": "text"
      },
      "source": [
        "Dot products are useful in a wide range of contexts. For example, given a set of weights $\\mathbf{w}$, the weighted sum of some values ${u}$ could be expressed as the dot product $\\mathbf{u}^T \\mathbf{w}$. When the weights are non-negative and sum to one $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$, the dot product expresses a *weighted average*. When two vectors each have length one (we will discuss what *length* means below in the section on norms), dot products can also capture the cosine of the angle between them.\n",
        "\n",
        "## Matrix-vector products\n",
        "\n",
        "Now that we know how to calculate dot products we can begin to understand matrix-vector products. Let's start off by visualizing a matrix $A$ and a column vector $\\mathbf{x}$.\n",
        "\n",
        "$$A=\\begin{pmatrix}\n",
        " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
        " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
        "\\end{pmatrix},\\quad\\mathbf{x}=\\begin{pmatrix}\n",
        " x_{1}  \\\\\n",
        " x_{2} \\\\\n",
        "\\vdots\\\\\n",
        " x_{m}\\\\\n",
        "\\end{pmatrix} $$\n",
        "\n",
        "We can visualize the matrix in terms of its row vectors\n",
        "\n",
        "$$A=\n",
        "\\begin{pmatrix}\n",
        "\\mathbf{a}^T_{1} \\\\\n",
        "\\mathbf{a}^T_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^T_n \\\\\n",
        "\\end{pmatrix},$$\n",
        "\n",
        "where each $\\mathbf{a}^T_{i} \\in \\mathbb{R}^{m}$\n",
        "is a row vector representing the $i$-th row of the matrix $A$.\n",
        "\n",
        "Then the matrix vector product $\\mathbf{y} = A\\mathbf{x}$ is simply a column vector $\\mathbf{y} \\in \\mathbb{R}^n$ where each entry $y_i$ is the dot product $\\mathbf{a}^T_i \\mathbf{x}$.\n",
        "\n",
        "$$A\\mathbf{x}=\n",
        "\\begin{pmatrix}\n",
        "\\mathbf{a}^T_{1}  \\\\\n",
        "\\mathbf{a}^T_{2}  \\\\\n",
        " \\vdots  \\\\\n",
        "\\mathbf{a}^T_n \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        " x_{1}  \\\\\n",
        " x_{2} \\\\\n",
        "\\vdots\\\\\n",
        " x_{m}\\\\\n",
        "\\end{pmatrix}\n",
        "= \\begin{pmatrix}\n",
        " \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n",
        " \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n",
        "\\vdots\\\\\n",
        " \\mathbf{a}^T_{n} \\mathbf{x}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "So you can think of multiplication by a matrix $A\\in \\mathbb{R}^{n \\times m}$ as a transformation that projects vectors from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$.\n",
        "\n",
        "These transformations turn out to be remarkably useful. For example, we can represent rotations as multiplications by a square matrix. As we will see in subsequent chapters, we can also use matrix-vector products to describe the calculations of each layer in a neural network.\n",
        "\n",
        "Expressing matrix-vector products in code with ``tensor``, we use ``torch.mv()`` function. When we call ``torch.mv(A, x)`` with a matrix ``A`` and a vector ``x``, PyTorch knows to perform a matrix-vector product. Note that the column dimension of ``A`` must be the same as the dimension of ``x``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaMAUb-qEuEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.mv(A, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpesV2ldEuEK",
        "colab_type": "text"
      },
      "source": [
        "## Matrix-matrix multiplication\n",
        "\n",
        "If you have gotten the hang of dot products and matrix-vector multiplication, then matrix-matrix multiplications should be pretty straightforward.\n",
        "\n",
        "Say we have two matrices, $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$:\n",
        "\n",
        "$$A=\\begin{pmatrix}\n",
        " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
        " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
        "\\end{pmatrix},\\quad\n",
        "B=\\begin{pmatrix}\n",
        " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
        " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "To produce the matrix product $C = AB$, it's easiest to think of $A$ in terms of its row vectors and $B$ in terms of its column vectors:\n",
        "\n",
        "$$A=\n",
        "\\begin{pmatrix}\n",
        "\\mathbf{a}^T_{1} \\\\\n",
        "\\mathbf{a}^T_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^T_n \\\\\n",
        "\\end{pmatrix},\n",
        "\\quad B=\\begin{pmatrix}\n",
        " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Note here that each row vector $\\mathbf{a}^T_{i}$ lies in $\\mathbb{R}^k$ and that each column vector $\\mathbf{b}_j$ also lies in $\\mathbb{R}^k$.\n",
        "\n",
        "Then to produce the matrix product $C \\in \\mathbb{R}^{n \\times m}$ we simply compute each entry $c_{ij}$ as the dot product $\\mathbf{a}^T_i \\mathbf{b}_j$.\n",
        "\n",
        "$$C = AB = \\begin{pmatrix}\n",
        "\\mathbf{a}^T_{1} \\\\\n",
        "\\mathbf{a}^T_{2} \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{a}^T_n \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
        "\\end{pmatrix}\n",
        "= \\begin{pmatrix}\n",
        "\\mathbf{a}^T_{1} \\mathbf{b}_1 & \\mathbf{a}^T_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^T_{1} \\mathbf{b}_m \\\\\n",
        " \\mathbf{a}^T_{2}\\mathbf{b}_1 & \\mathbf{a}^T_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\mathbf{b}_m \\\\\n",
        " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
        "\\mathbf{a}^T_{n} \\mathbf{b}_1 & \\mathbf{a}^T_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\mathbf{b}_m\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix. We can compute matrix-matrix products in PyTorch by using ``torch.mm()``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwzhJF5DEuEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "B = torch.ones(size=(4, 3))\n",
        "torch.mm(A, B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urkOSFagEuEM",
        "colab_type": "text"
      },
      "source": [
        "## Norms\n",
        "\n",
        "Before we can start implementing models,\n",
        "there is one last concept we are going to introduce.\n",
        "Some of the most useful operators in linear algebra are norms.\n",
        "Informally, they tell us how big a vector or matrix is.\n",
        "We represent norms with the notation $\\|\\cdot\\|$.\n",
        "The $\\cdot$ in this expression is just a placeholder.\n",
        "For example, we would represent the norm of a vector $\\mathbf{x}$\n",
        "or matrix $A$ as $\\|\\mathbf{x}\\|$ or $\\|A\\|$, respectively.\n",
        "\n",
        "All norms must satisfy a handful of properties:\n",
        "\n",
        "1. $\\|\\alpha A\\| = |\\alpha| \\|A\\|$\n",
        "1. $\\|A + B\\| \\leq \\|A\\| + \\|B\\|$\n",
        "1. $\\|A\\| \\geq 0$\n",
        "1. If $\\forall {i,j}, a_{ij} = 0$, then $\\|A\\|=0$\n",
        "\n",
        "To put it in words, the first rule says\n",
        "that if we scale all the components of a matrix or vector\n",
        "by a constant factor $\\alpha$,\n",
        "its norm also scales by the *absolute value*\n",
        "of the same constant factor.\n",
        "The second rule is the familiar triangle inequality.\n",
        "The third rule simply says that the norm must be non-negative.\n",
        "That makes sense, in most contexts the smallest *size* for anything is 0.\n",
        "The final rule basically says that the smallest norm is achieved by a matrix or vector consisting of all zeros.\n",
        "It is possible to define a norm that gives zero norm to nonzero matrices,\n",
        "but you cannot give nonzero norm to zero matrices.\n",
        "That may seem like a mouthful, but if you digest it then you probably have grepped the important concepts here.\n",
        "\n",
        "If you remember Euclidean distances (think Pythagoras' theorem) from grade school,\n",
        "then non-negativity and the triangle inequality might ring a bell.\n",
        "You might notice that norms sound a lot like measures of distance.\n",
        "\n",
        "In fact, the Euclidean distance $\\sqrt{x_1^2 + \\cdots + x_n^2}$ is a norm.\n",
        "Specifically it is the $\\ell_2$-norm.\n",
        "An analogous computation,\n",
        "performed over the entries of a matrix, e.g. $\\sqrt{\\sum_{i,j} a_{ij}^2}$,\n",
        "is called the Frobenius norm.\n",
        "More often, in machine learning we work with the squared $\\ell_2$ norm (notated $\\ell_2^2$).\n",
        "We also commonly work with the $\\ell_1$ norm.\n",
        "The $\\ell_1$ norm is simply the sum of the absolute values.\n",
        "It has the convenient property of placing less emphasis on outliers.\n",
        "\n",
        "To calculate the $\\ell_2$ norm, we can just call ``torch.norm()``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Woc_xdqCEuEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoSQgzlQEuER",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Write a function that calculates the $\\ell_1$ norm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gob3Kas1EuER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L1_norm(x):\n",
        "  ## write your code below\n",
        "  \n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJDBuo88EuEV",
        "colab_type": "text"
      },
      "source": [
        "## Norms and objectives\n",
        "\n",
        "While we do not want to get too far ahead of ourselves, we do want you to anticipate why these concepts are useful.\n",
        "In machine learning we are often trying to solve optimization problems: *Maximize* the probability assigned to observed data. *Minimize* the distance between predictions and the ground-truth observations. Assign vector representations to items (like words, products, or news articles) such that the distance between similar items is minimized, and the distance between dissimilar items is maximized. Oftentimes, these objectives, perhaps the most important component of a machine learning algorithm (besides the data itself), are expressed as norms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJWGSVtwEuEW",
        "colab_type": "text"
      },
      "source": [
        "## Intermediate linear algebra\n",
        "\n",
        "If you have made it this far, and understand everything that we have covered,\n",
        "then honestly, you *are* ready to begin modeling.\n",
        "If you are feeling antsy, this is a perfectly reasonable place to move on.\n",
        "You already know nearly all of the linear algebra required\n",
        "to implement a number of many practically useful models\n",
        "and you can always circle back when you want to learn more.\n",
        "\n",
        "But there is a lot more to linear algebra, even as concerns machine learning.\n",
        "At some point, if you plan to make a career in machine learning,\n",
        "you will need to know more than what we have covered so far.\n",
        "In the rest of this chapter, we introduce some useful, more advanced concepts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWQpaTwcEuEW",
        "colab_type": "text"
      },
      "source": [
        "### Basic vector properties\n",
        "\n",
        "Vectors are useful beyond being data structures to carry numbers.\n",
        "In addition to reading and writing values to the components of a vector,\n",
        "and performing some useful mathematical operations,\n",
        "we can analyze vectors in some interesting ways.\n",
        "\n",
        "One important concept is the notion of a vector space.\n",
        "Here are the conditions that make a vector space:\n",
        "\n",
        "* **Additive axioms** (we assume that x,y,z are all vectors):\n",
        "  $x+y = y+x$ and $(x+y)+z = x+(y+z)$ and $0+x = x+0 = x$ and $(-x) + x = x + (-x) = 0$.\n",
        "* **Multiplicative axioms** (we assume that x is a vector and a, b are scalars):\n",
        "  $0 \\cdot x = 0$ and $1 \\cdot x = x$ and $(a b) x = a (b x)$.\n",
        "* **Distributive axioms** (we assume that x and y are vectors and a, b are scalars):\n",
        "  $a(x+y) = ax + ay$ and $(a+b)x = ax +bx$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq41x1WXEuEX",
        "colab_type": "text"
      },
      "source": [
        "### Special matrices\n",
        "\n",
        "There are a number of special matrices that we will use throughout this tutorial. Let's look at them in a bit of detail:\n",
        "\n",
        "* **Symmetric Matrix** These are matrices where the entries below and above the diagonal are the same. In other words, we have that $M^\\top = M$. An example of such matrices are those that describe pairwise distances, i.e. $M_{ij} = \\|x_i - x_j\\|$. Likewise, the Facebook friendship graph can be written as a symmetric matrix where $M_{ij} = 1$ if $i$ and $j$ are friends and $M_{ij} = 0$ if they are not. Note that the *Twitter* graph is asymmetric - $M_{ij} = 1$, i.e. $i$ following $j$ does not imply that $M_{ji} = 1$, i.e. $j$ following $i$.\n",
        "* **Antisymmetric Matrix** These matrices satisfy $M^\\top = -M$. Note that any square matrix can always be decomposed into a symmetric and into an antisymmetric matrix by using $M = \\frac{1}{2}(M + M^\\top) + \\frac{1}{2}(M - M^\\top)$.\n",
        "* **Diagonally Dominant Matrix** These are matrices where the off-diagonal elements are small relative to the main diagonal elements. In particular we have that $M_{ii} \\geq \\sum_{j \\neq i} M_{ij}$ and $M_{ii} \\geq \\sum_{j \\neq i} M_{ji}$. If a matrix has this property, we can often approximate $M$ by its diagonal. This is often expressed as $\\mathrm{diag}(M)$.\n",
        "* **Positive Definite Matrix** These are matrices that have the nice property where $x^\\top M x > 0$ whenever $x \\neq 0$. Intuitively, they are a generalization of the squared norm of a vector $\\|x\\|^2 = x^\\top x$. It is easy to check that whenever $M = A^\\top A$, this holds since there $x^\\top M x = x^\\top A^\\top A x = \\|A x\\|^2$. There is a somewhat more profound theorem which states that all positive definite matrices can be written in this form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrByOQ7SEuEY",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In just a few pages (or one Jupyter notebook) we have taught you all the linear algebra you will need to understand a good chunk of neural networks. Of course there is a *lot* more to linear algebra. And a lot of that math *is* useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensional structure in real-world datasets. There are entire subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning. And we believe you will be much more inclined to learn more mathematics once you have gotten your hands dirty deploying useful machine learning models on real datasets. So while we reserve the right to introduce more math much later on, we will wrap up this chapter here.\n",
        "\n",
        "If you are eager to learn more about linear algebra, here are some of our favorite resources on the topic\n",
        "\n",
        "* For a solid primer on basics, check out Gilbert Strang's book [Introduction to Linear Algebra](http://math.mit.edu/~gs/linearalgebra/)\n",
        "* Zico Kolter's [Linear Algebra Review and Reference](http://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siMlXnAnEuEY",
        "colab_type": "text"
      },
      "source": [
        "# Probability and Statistics\n",
        "\n",
        "In some form or another, machine learning is all about making predictions.\n",
        "We might want to predict the *probability* of a patient suffering a heart attack in the next year, given their clinical history. In anomaly detection, we might want to assess how *likely* a set of readings from an airplane's jet engine would be, were it operating normally. In reinforcement learning, we want an agent to act intelligently in an environment. This means we need to think about the probability of getting a high reward under each of the available action. And when we build recommender systems we also need to think about probability. For example, say *hypothetically* that we work for a large online bookseller. We might want to estimate the probability that a particular user would buy a particular book. For this we need to use the language of probability and statistics. Entire courses, majors, theses, careers, and even departments, are devoted to probability. So naturally, our goal in this section isn't to teach the whole subject. Instead we hope to get you off the ground, to teach you just enough that you can start building your first machine\n",
        "learning models, and to give you enough of a flavor for the subject that you can begin to explore it on your own if you wish.\n",
        "\n",
        "\n",
        "We've already invoked probabilities in previous sections without articulating what precisely they are or giving a concrete example. Let's get more serious now by considering the problem of distinguishing cats and dogs based on photographs. This might sound simple but it's actually a formidable challenge. To start with, the difficulty of the problem may depend on the resolution of the image.\n",
        "\n",
        "| 10px | 20px | 40px | 80px | 160px |\n",
        "|:----:|:----:|:----:|:----:|:-----:|\n",
        "|![](../img/whitecat10.jpg)|![](../img/whitecat20.jpg)|![](../img/whitecat40.jpg)|![](../img/whitecat80.jpg)|![](../img/whitecat160.jpg)|\n",
        "|![](../img/whitedog10.jpg)|![](../img/whitedog20.jpg)|![](../img/whitedog40.jpg)|![](../img/whitedog80.jpg)|![](../img/whitedog160.jpg)|\n",
        "\n",
        "While it's easy for humans to recognize cats and dogs at 320 pixel resolution,\n",
        "it becomes challenging at 40 pixels and next to impossible at 10 pixels. In\n",
        "other words, our ability to tell cats and dogs apart at a large distance (and thus low resolution) might approach uninformed guessing. Probability gives us a\n",
        "formal way of reasoning about our level of certainty. If we are completely sure\n",
        "that the image depicts a cat, we say that the *probability* that the corresponding label $l$ is $\\mathrm{cat}$, denoted $P(l=\\mathrm{cat})$ equals\n",
        "1.0. If we had no evidence to suggest that $l =\\mathrm{cat}$ or that $l =\n",
        "\\mathrm{dog}$, then we might say that the two possibilities were equally\n",
        "$likely$ expressing this as $P(l=\\mathrm{cat}) = 0.5$. If we were reasonably\n",
        "confident, but not sure that the image depicted a cat, we might assign a\n",
        "probability $.5  < P(l=\\mathrm{cat}) < 1.0$.\n",
        "\n",
        "Now consider a second case: given some weather monitoring data, we want to predict the probability that it will rain in Taipei tomorrow. If it's summertime, the rain might come with probability $.5$. In both cases, we have some value of interest. And in both cases we are uncertain about the outcome.\n",
        "But there's a key difference between the two cases. In this first case, the image is in fact either a dog or a cat, we just don't know which. In the second case, the outcome may actually be a random event, if you believe in such things (and most physicists do). So probability is a flexible language for reasoning about our level of certainty, and it can be applied effectively in a broad set of contexts.\n",
        "\n",
        "## Basic probability theory\n",
        "\n",
        "Say that we cast a die and want to know what the chance is of seeing a $1$ rather than another digit. If the die is fair, all six outcomes $\\mathcal{X} = \\{1, \\ldots, 6\\}$ are equally likely to occur, and thus we would see a $1$ in $1$ out of $6$ cases. Formally we state that $1$ occurs with probability $\\frac{1}{6}$.\n",
        "\n",
        "For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted. The only way to investigate the die is by casting it many times and recording the outcomes. For each cast of the die, we'll observe a value $\\{1, 2, \\ldots, 6\\}$. Given these outcomes, we want to investigate the probability of observing each outcome.\n",
        "\n",
        "One natural approach for each value is to take the\n",
        "individual count for that value and to divide it by the total number of tosses.\n",
        "This gives us an *estimate* of the probability of a given event. The law of\n",
        "large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. Before going into the details of what's going here, let's try it out.\n",
        "\n",
        "To start, let's import the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdkJuzBBEuEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.distributions.multinomial import Multinomial\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfGg5sv7EuEi",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll want to be able to cast the die. In statistics we call this process\n",
        "of drawing examples from probability distributions *sampling*.\n",
        "The distribution\n",
        "which assigns probabilities to a number of discrete choices is called the\n",
        "*multinomial* distribution. We'll give a more formal definition of\n",
        "*distribution* later, but at a high level, think of it as just an assignment of\n",
        "probabilities to events. In PyTorch, we can sample from the multinomial\n",
        "distribution via the aptly named `torch.distributions.multinomial.Multinomial` function which we have imported above. To draw a single\n",
        "sample, we simply pass in a vector of probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii2KenW4EuEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "probabilities = torch.ones([6], dtype = torch.float64) / 6\n",
        "distribution = Multinomial(total_count = 1, probs = probabilities)\n",
        "distribution.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4nwOuFXEuEk",
        "colab_type": "text"
      },
      "source": [
        "If you want to run the sampler a bunch of times, you can simply set `total_count` to some number. As with estimating the fairness of a die, we often want to\n",
        "generate many samples from the same distribution. It would be unbearably slow to\n",
        "do this with a Python `for` loop, so `torch.distributions.multinomial.Multinomial` supports drawing\n",
        "multiple samples at once. For example if we set `total_count` to 100, we would get a tensor showing the number of times each face of the die appeared (sum of the tensor would be 100)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqTltFQ6EuEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((Multinomial(total_count = 10, probs = probabilities)).sample())\n",
        "print((Multinomial(total_count = 100, probs = probabilities)).sample())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r4ujoH4EuEn",
        "colab_type": "text"
      },
      "source": [
        "Now that we know how to sample rolls of a die, we can simulate 1000 rolls. We\n",
        "can then go through and count, after each of the 1000 rolls, how many times each\n",
        "number was rolled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdejUfewEuEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_rolls = 1000\n",
        "rolls = Multinomial(total_count = 1, probs = probabilities)\n",
        "counts = torch.zeros(6, 1000, dtype = torch.double)\n",
        "totals = torch.zeros(6, dtype = torch.double)\n",
        "for i in range(num_rolls):\n",
        "    totals = totals + rolls.sample()\n",
        "    counts[:, i] = totals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6skfU7nWEuEp",
        "colab_type": "text"
      },
      "source": [
        "To start, we can inspect the final tally at the end of $1000$ rolls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4a7aI6QEuEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totals / 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3asBAgd0EuEs",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the lowest estimated probability for any of the numbers is about $0.146$ and the highest estimated probability is $0.176$. Because we generated the data from a fair die, we know that each number actually has probability of $1/6$, roughly $0.167$, so these estimates are pretty good. We can also visualize how these probabilities converge over time towards reasonable estimates.\n",
        "\n",
        "To start let's take a look at the `counts` array which has shape `(6, 1000)`.\n",
        "For each time step (out of 1000), `counts` says how many times each of the numbers has shown up. So we can normalize each $j$-th column of the counts vector by the number of tosses to give the `current` estimated probabilities at that time. The counts object looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VPWw-K0EuEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYYKEy-PEuEu",
        "colab_type": "text"
      },
      "source": [
        "Normalizing by the number of tosses, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFqwdm7vEuEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.arange(1000, dtype = torch.double).reshape((1,1000)) + 1\n",
        "estimates = counts / x\n",
        "print(estimates[:,0])\n",
        "print(estimates[:,1])\n",
        "print(estimates[:,100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s0xSJ0gEuEw",
        "colab_type": "text"
      },
      "source": [
        "As you can see, after the first toss of the die, we get the extreme estimate\n",
        "that one of the numbers will be rolled with probability $1.0$ and that the\n",
        "others have probability $0$. After $100$ rolls, things already look a bit more\n",
        "reasonable. We can visualize this convergence by using the plotting package\n",
        "`matplotlib`. If you don't have it installed, now would be a good time to\n",
        "[install it](https://matplotlib.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy_5HAXbEuEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display.set_matplotlib_formats('svg')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(6):\n",
        "    plt.plot(estimates[i, :].numpy(), label=(\"P(die=\" + str(i) +\")\"))\n",
        "\n",
        "plt.axhline(y=0.16666, color='black', linestyle='dashed')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ9I5VkvEuEy",
        "colab_type": "text"
      },
      "source": [
        "Each solid curve corresponds to one of the six values of the die and gives our estimated probability that the die turns up that value as assessed after each of the 1000 turns. The dashed black line gives the true underlying probability. As we get more data, the solid curves converge towards the true answer.\n",
        "\n",
        "In our example of casting a die, we introduced the notion of a **random variable**. A random variable, which we denote here as $X$ can be pretty much any quantity and is not deterministic. Random variables could take one value among a set of possibilities. We denote sets with brackets, e.g., $\\{\\mathrm{cat}, \\mathrm{dog}, \\mathrm{rabbit}\\}$. The items contained in the set are called *elements*, and we can say that an element $x$ is *in* the set S, by writing $x \\in S$. The symbol $\\in$ is read as \"in\" and denotes membership. For instance, we could truthfully say $\\mathrm{dog} \\in \\{\\mathrm{cat}, \\mathrm{dog}, \\mathrm{rabbit}\\}$. When dealing with the rolls of die, we are concerned with a variable $X \\in \\{1, 2, 3, 4, 5, 6\\}$.\n",
        "\n",
        "Note that there is a subtle difference between discrete random variables, like the sides of a dice, and continuous ones, like the weight and the height of a person. There's little point in asking whether two people have exactly the same height. If we take precise enough measurements you'll find that no two people on the planet have the exact same height. In fact, if we take a fine enough measurement, you will not have the same height when you wake up and when you go to sleep. So there's no purpose in asking about the probability\n",
        "that someone is $2.00139278291028719210196740527486202$ meters tall. Given the world population of humans the probability is virtually 0. It makes more sense in this case to ask whether someone's height falls into a given interval, say between 1.99 and 2.01 meters. In these cases we quantify the likelihood that we see a value as a *density*. The height of exactly 2.0 meters has no probability, but nonzero density. In the interval between any two different heights we have nonzero probability.\n",
        "\n",
        "\n",
        "There are a few important axioms of probability that you'll want to remember:\n",
        "\n",
        "* For any event $z$, the probability is never negative, i.e. $\\Pr(Z=z) \\geq 0$.\n",
        "* For any two events $Z=z$ and $X=x$ the union is no more likely than the sum of the individual events, i.e. $\\Pr(Z=z \\cup X=x) \\leq \\Pr(Z=z) + \\Pr(X=x)$.\n",
        "* For any random variable, the probabilities of all the values it can take must sum to 1, i.e. $\\sum_{i=1}^n \\Pr(Z=z_i) = 1$.\n",
        "* For any two *mutually exclusive* events $Z=z$ and $X=x$, the probability that either happens is equal to the sum of their individual probabilities, that is $\\Pr(Z=z \\cup X=x) = \\Pr(Z=z) + \\Pr(X=x)$.\n",
        "\n",
        "## Dealing with multiple random variables\n",
        "Very often, we'll want to consider more than one random variable at a time.\n",
        "For instance, we may want to model the relationship between diseases and symptoms. Given a disease and symptom, say 'flu' and 'cough', either may or may not occur in a patient with some probability. While we hope that the probability of both would be close to zero, we may want to estimate these probabilities and their relationships to each other so that we may apply our inferences to effect better medical care.\n",
        "\n",
        "As a more complicated example, images contain millions of pixels, thus millions of random variables. And in many cases images will come with a\n",
        "label, identifying objects in the image. We can also think of the label as a\n",
        "random variable. We can even think of all the metadata as random variables\n",
        "such as location, time, aperture, focal length, ISO, focus distance, camera type, etc. All of these are random variables that occur jointly. When we deal with multiple random variables, there are several quantities of interest. The first is called the joint distribution $\\Pr(A, B)$. Given any elements $a$ and $b$, the joint distribution lets us answer, what is the probability that $A=a$ and $B=b$ simultaneously? Note that for any values $a$ and $b$, $\\Pr(A=a,B=b) \\leq \\Pr(A=a)$.\n",
        "\n",
        "This has to be the case, since for $A$ and $B$ to happen, $A$ has to happen *and* $B$ also has to happen (and vice versa). Thus $A,B$ cannot be more likely than $A$ or $B$ individually. This brings us to an interesting ratio: $0 \\leq \\frac{\\Pr(A,B)}{\\Pr(A)} \\leq 1$. We call this a **conditional probability**\n",
        "and denote it by $\\Pr(B | A)$, the probability that $B$ happens, provided that\n",
        "$A$ has happened.\n",
        "\n",
        "Using the definition of conditional probabilities, we can derive one of the most useful and celebrated equations in statistics—Bayes' theorem.\n",
        "It goes as follows: By construction, we have that $\\Pr(A, B) = \\Pr(B | A) \\Pr(A)$. By symmetry, this also holds for $\\Pr(A,B) = \\Pr(A | B) \\Pr(B)$. Solving for one of the conditional variables we get:\n",
        "\n",
        "$$\\Pr(A | B) = \\frac{\\Pr(B | A) \\Pr(A)}{\\Pr(B)}$$\n",
        "\n",
        "This is very useful if we want to infer one thing from another, say cause and effect but we only know the properties in the reverse direction. One important operation that we need, to make this work, is **marginalization**, i.e., the operation of determining $\\Pr(A)$ and $\\Pr(B)$ from $\\Pr(A,B)$. We can see that the probability of seeing $A$ amounts to accounting for all possible choices of $B$ and aggregating the joint probabilities over all of them, i.e.\n",
        "\n",
        "$$\\Pr(A) = \\sum_{B'} \\Pr(A,B') \\text{ and\n",
        "} \\Pr(B) = \\sum_{A'} \\Pr(A',B)$$\n",
        "\n",
        "Another useful property to check for is **dependence** vs. **independence**.\n",
        "Independence is when the occurrence of one event does not reveal any information about the occurrence of the other. In this case $\\Pr(B | A) = \\Pr(B)$. Statisticians typically express this as $A \\perp\\!\\!\\!\\perp B$. From Bayes' Theorem, it follows immediately that also $\\Pr(A | B) = \\Pr(A)$. In all other cases we call $A$ and $B$ dependent. For instance, two successive rolls of a die are independent. On the other hand, the position of a light switch and the brightness in the room are not (they are not perfectly deterministic, though, since we could always have a broken lightbulb, power failure, or a broken switch).\n",
        "\n",
        "Let's put our skills to the test. Assume that a doctor administers an AIDS test to a patient. This test is fairly accurate and it fails only with 1% probability if the patient is healthy by reporting him as diseased. Moreover,\n",
        "it never fails to detect HIV if the patient actually has it. We use $D$ to indicate the diagnosis and $H$ to denote the HIV status. Written as a table the outcome $\\Pr(D | H)$ looks as follows:\n",
        "\n",
        "|outcome| HIV positive | HIV negative |\n",
        "|:------------|-------------:|-------------:|\n",
        "|Test positive|            1 |         0.01 |\n",
        "|Test negative|            0 |         0.99 |\n",
        "\n",
        "Note that the column sums are all one (but the row sums aren't), since the conditional probability needs to sum up to $1$, just like the probability. Let us work out the probability of the patient having AIDS if the test comes back positive. Obviously this is going to depend on how common the disease is, since it affects the number of false alarms. Assume that the population is quite healthy, e.g. $\\Pr(\\text{HIV positive}) = 0.0015$. To apply Bayes' Theorem, we need to determine\n",
        "$$\\begin{aligned}\n",
        "\\Pr(\\text{Test positive}) =& \\Pr(D=1 | H=0) \\Pr(H=0) + \\Pr(D=1\n",
        "| H=1) \\Pr(H=1) \\\\\n",
        "=& 0.01 \\cdot 0.9985 + 1 \\cdot 0.0015 \\\\\n",
        "=& 0.011485\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus, we get\n",
        "\n",
        "$$\\begin{aligned} \\Pr(H = 1 | D = 1) =& \\frac{\\Pr(D=1 | H=1) \\Pr(H=1)}{\\Pr(D=1)} \\\\ =& \\frac{1 \\cdot 0.0015}{0.011485} \\\\ =& 0.131 \\end{aligned} $$\n",
        "\n",
        "In other words, there's only a 13.1% chance that the patient actually has AIDS, despite using a test that is 99% accurate. As we can see, statistics can be quite counterintuitive.\n",
        "\n",
        "## Conditional independence\n",
        "What should a patient do upon receiving such terrifying news? Likely, he/she\n",
        "would ask the physician to administer another test to get clarity. The second\n",
        "test has different characteristics (it isn't as good as the first one).\n",
        "\n",
        "|outcome |  HIV positive |  HIV negative |\n",
        "|:------------|--------------:|--------------:|\n",
        "|Test positive|          0.98 |          0.03 |\n",
        "|Test negative|          0.02 |          0.97 |\n",
        "\n",
        "Unfortunately, the second test comes back positive, too. Let us work out the requisite probabilities to invoke Bayes' Theorem.\n",
        "\n",
        "* $\\Pr(D_1 = 1 \\text{ and } D_2 = 1 | H = 0) = 0.01 \\cdot 0.03 = 0.0003$\n",
        "* $\\Pr(D_1 = 1 \\text{ and } D_2 = 1 | H = 1) = 1 \\cdot 0.98 = 0.98$\n",
        "* $\\Pr(D_1 = 1 \\text{ and } D_2 = 1) = 0.0003 \\cdot 0.9985 + 0.98 \\cdot 0.0015 = 0.00176955$\n",
        "* $\\Pr(H = 1 | D_1 = 1 \\text{ and } D_2 = 1) = \\frac{0.98 \\cdot 0.0015}{0.00176955} = 0.831$\n",
        "\n",
        "That is, the second test allowed us to gain much higher confidence that not all is well. Despite the second test being considerably less accurate than the first one, it still improved our estimate quite a bit. You might ask, *why couldn't we just run the first test a second time?* After all, the first test was more accurate. The reason is that we needed a second test whose result is *independent* of the first test (given the true diagnosis). In other words, we made the tacit assumption that $\\Pr(D_1, D_2 | H) = \\Pr(D_1 | H) \\Pr(D_2 | H)$. Statisticians call such random variables **conditionally independent**. This is expressed as $D_1 \\perp\\!\\!\\!\\perp D_2  | H$.\n",
        "\n",
        "## Sampling\n",
        "\n",
        "Often, when working with probabilistic models, we'll want not just to estimate distributions from data, but also to generate data by sampling from distributions. One of the simplest ways to sample random numbers is to invoke the `random` method from Python's `random` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X36xD3u8EuE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "for i in range(10):\n",
        "    print(random.random())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHhfzn99EuE2",
        "colab_type": "text"
      },
      "source": [
        "### Uniform Distribution\n",
        "\n",
        "These numbers likely *appear* random. Note that their range is between 0 and 1 and they are evenly distributed. Because these numbers are generated by default from the uniform distribution, there should be no two sub-intervals of $[0,1]$ of equal size where numbers are more likely to lie in one interval than the other. In other words, the chances of any of these numbers to fall into the interval $[0.2,0.3)$ are the same as in the interval $[.593264, .693264)$. In fact, these numbers are pseudo-random, and the computer generates them by first producing a random integer and then dividing it by its maximum range. To sample random integers directly, we can run the following snippet, which generates integers in the range between 1 and 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vru9RzUyEuE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "    print(random.randint(1, 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YDsoXq-EuE5",
        "colab_type": "text"
      },
      "source": [
        "How might we check that ``randint`` is really uniform? Intuitively, the best\n",
        "strategy would be to run sampler many times, say 1 million, and then count the\n",
        "number of times it generates each value to ensure that the results are approximately uniform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHYHoufrEuE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "counts = np.zeros(100)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(11, 6), sharex=True)\n",
        "axes = axes.reshape(6)\n",
        "\n",
        "# Mangle subplots such that we can index them in a linear fashion rather than\n",
        "# a 2D grid\n",
        "\n",
        "for i in range(1, 1000001):\n",
        "    counts[random.randint(0, 99)] += 1\n",
        "    if i in [10, 100, 1000, 10000, 100000, 1000000]:\n",
        "        axes[int(math.log10(i))-1].bar(np.arange(1, 101), counts)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5jbTtN9EuE9",
        "colab_type": "text"
      },
      "source": [
        "We can see from these figures that the initial number of counts looks *strikingly* uneven. If we sample fewer than 100 draws from a distribution over\n",
        "100 outcomes this should be expected. But even for 1000 samples there is a\n",
        "significant variability between the draws. What we are really aiming for is a\n",
        "situation where the probability of drawing a number $x$ is given by $p(x)$.\n",
        "\n",
        "### The categorical distribution\n",
        "\n",
        "Drawing from a uniform distribution over a set of 100 outcomes is simple. But what if we have nonuniform probabilities? Let's start with a simple case, a biased coin which comes up heads with probability 0.35 and tails with probability 0.65. A simple way to sample from that is to generate a uniform random variable over $[0,1]$ and if the number is less than $0.35$, we output heads and otherwise we generate tails. Let's try this out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXvWMBnOEuE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of samples\n",
        "n = 1000000\n",
        "y = np.random.uniform(0, 1, n)\n",
        "x = np.arange(1, n+1)\n",
        "# Count number of occurrences and divide by the number of total draws\n",
        "p0 = np.cumsum(y < 0.35) / x\n",
        "p1 = np.cumsum(y >= 0.35) / x\n",
        "\n",
        "plt.figure(figsize=(11, 6))\n",
        "plt.semilogx(x, p0)\n",
        "plt.semilogx(x, p1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m81Wovq4EuFA",
        "colab_type": "text"
      },
      "source": [
        "As we can see, on average, this sampler will generate 35% zeros and 65% ones.\n",
        "Now what if we have more than two possible outcomes? We can simply generalize\n",
        "this idea as follows. Given any probability distribution, e.g. $p = [0.1, 0.2, 0.05, 0.3, 0.25, 0.1]$ we can compute its cumulative distribution (python's ``cumsum`` will do this for you) $F = [0.1, 0.3, 0.35, 0.65, 0.9, 1]$. Once we have this we draw a random variable $x$ from the uniform distribution $U[0,1]$ and then find the interval where $F[i-1] \\leq x < F[i]$. We then return $i$ as the sample. By construction, the chances of hitting interval $[F[i-1], F[i])$ has probability $p(i)$.\n",
        "\n",
        "Note that there are many more efficient algorithms for sampling than the one above. For instance, binary search over $F$ will run in $O(\\log n)$ time for $n$ random variables. There are even more clever algorithms, such as the [Alias\n",
        "Method](https://en.wikipedia.org/wiki/Alias_method) to sample in constant time,\n",
        "after $O(n)$ preprocessing.\n",
        "\n",
        "### The Normal distribution\n",
        "\n",
        "The standard Normal distribution (aka the standard Gaussian distribution) is given by $p(x) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{1}{2} x^2\\right)$. Let's plot it to get a feel for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr8DyEUhEuFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-10, 10, 0.01)\n",
        "p = (1/math.sqrt(2 * math.pi)) * np.exp(-0.5 * x**2)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x, p)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLe-T2meEuFC",
        "colab_type": "text"
      },
      "source": [
        "Sampling from this distribution is less trivial. First off, the support is\n",
        "infinite, that is, for any $x$ the density $p(x)$ is positive. Secondly, the\n",
        "density is nonuniform. There are many tricks for sampling from it - the key idea in all algorithms is to stratify $p(x)$ in such a way as to map it to the\n",
        "uniform distribution $U[0,1]$. One way to do this is with the probability\n",
        "integral transform.\n",
        "\n",
        "Denote by $F(x) = \\int_{-\\infty}^x p(z) dz$ the cumulative distribution function (CDF) of $p$. This is in a way the continuous version of the cumulative sum that we used previously. In the same way we can now define the inverse map $F^{-1}(\\xi)$, where $\\xi$ is drawn uniformly. Unlike previously where we needed to find the correct interval for the vector $F$ (i.e. for the piecewise constant function), we now invert the function $F(x)$.\n",
        "\n",
        "In practice, this is slightly more tricky since inverting the CDF is hard in the case of a Gaussian. It turns out that the *twodimensional* integral is much easier to deal with, thus yielding two normal random variables than one, albeit at the price of two uniformly distributed ones. For now, suffice it to say that there are built-in algorithms to address this.\n",
        "\n",
        "The normal distribution has yet another desirable property. In a way all distributions converge to it, if we only average over a sufficiently large number of draws from any other distribution. To understand this in a bit more detail, we need to introduce three important things: expected values, means and variances.\n",
        "\n",
        "* The expected value $\\mathbf{E}_{x \\sim p(x)}[f(x)]$ of a function $f$ under a distribution $p$ is given by the integral $\\int_x p(x) f(x) dx$. That is, we average over all possible outcomes, as given by $p$.\n",
        "* A particularly important expected value is\n",
        "that for the function $f(x) = x$, i.e. $\\mu := \\mathbf{E}_{x \\sim p(x)}[x]$. It\n",
        "provides us with some idea about the typical values of $x$.\n",
        "* Another important quantity is the variance, i.e. the typical deviation from the mean $\\sigma^2 := \\mathbf{E}_{x \\sim p(x)}[(x-\\mu)^2]$. Simple math shows (check it as an exercise) that $\\sigma^2 = \\mathbf{E}_{x \\sim p(x)}[x^2] - \\mathbf{E}^2_{x \\sim p(x)}[x]$.\n",
        "\n",
        "The above allows us to change both mean and variance of random variables. Quite obviously for some random variable $x$ with mean $\\mu$, the random variable $x + c$ has mean $\\mu + c$. Moreover, $\\gamma x$ has the variance $\\gamma^2 \\sigma^2$. Applying this to the normal distribution we see that one with mean $\\mu$ and variance $\\sigma^2$ has the form $p(x) = \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x-\\mu)^2\\right)$. Note the scaling factor $\\frac{1}{\\sigma}$—it arises from the fact that if we stretch the distribution by $\\sigma$, we need to lower it by $\\frac{1}{\\sigma}$ to retain the same probability mass (i.e. the weight under the distribution always needs to integrate out to 1).\n",
        "\n",
        "Now we are ready to state one of the most fundamental theorems in statistics, the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). It states that for sufficiently well-behaved random variables, in particular random variables with well-defined mean and variance, the sum tends toward a normal distribution. To get some idea, let's repeat the experiment described in the beginning, but now using random variables with integer values of $\\{0, 1, 2\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBT_hb1lEuFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate 10 random sequences of 10,000 uniformly distributed random variables\n",
        "tmp = np.random.uniform(size=(10000,10))\n",
        "x = 1.0 * (tmp > 0.3) + 1.0 * (tmp > 0.8)\n",
        "mean = 1 * 0.5 + 2 * 0.2\n",
        "variance = 1 * 0.5 + 4 * 0.2 - mean**2\n",
        "print('mean {}, variance {}'.format(mean, variance))\n",
        "\n",
        "# Cumulative sum and normalization\n",
        "y = np.arange(1,10001).reshape(10000,1)\n",
        "z = np.cumsum(x,axis=0) / y\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "for i in range(10):\n",
        "    plt.semilogx(y,z[:,i])\n",
        "\n",
        "plt.semilogx(y,(variance**0.5) * np.power(y,-0.5) + mean,'r')\n",
        "plt.semilogx(y,-(variance**0.5) * np.power(y,-0.5) + mean,'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0yu-0J9EuFF",
        "colab_type": "text"
      },
      "source": [
        "This looks very similar to the initial example, at least in the limit of averages of large numbers of variables. This is confirmed by theory. Denote by\n",
        "mean and variance of a random variable the quantities\n",
        "\n",
        "$$\\mu[p] := \\mathbf{E}_{x \\sim p(x)}[x] \\text{ and } \\sigma^2[p] := \\mathbf{E}_{x \\sim p(x)}[(x - \\mu[p])^2]$$\n",
        "\n",
        "Then we have that $\\lim_{n\\to \\infty} \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{x_i - \\mu}{\\sigma} \\to \\mathcal{N}(0, 1)$. In other words, regardless of what we started out with, we will always converge to a Gaussian. This is one of the reasons why Gaussians are so popular in statistics.\n",
        "\n",
        "\n",
        "### More distributions\n",
        "\n",
        "Many more useful distributions exist. If you're interested in going deeper, we recommend consulting a dedicated book on statistics or looking up some common distributions on Wikipedia for further detail. Some important distirbutions to be aware of include:\n",
        "\n",
        "* **Binomial Distribution** It is used to describe the distribution over multiple draws from the same distribution, e.g. the number of heads when tossing a biased coin (i.e. a coin with probability $\\pi \\in [0, 1]$ of returning heads) 10 times. The binomial probability is given by $p(x) = {n \\choose x} \\pi^x (1-\\pi)^{n-x}$.\n",
        "* **Multinomial Distribution** Often, we are concerned with more than two\n",
        "outcomes, e.g. when rolling a dice multiple times. In this case, the\n",
        "distribution is given by $p(x) = \\frac{n!}{\\prod_{i=1}^k x_i!} \\prod_{i=1}^k \\pi_i^{x_i}$.\n",
        "* **Poisson Distribution** This distribution models the occurrence of point events that happen with a given rate, e.g. the number of raindrops arriving within a given amount of time in an area (weird fact - the number of Prussian soldiers being killed by horses kicking them followed that distribution). Given a rate $\\lambda$, the number of occurrences is given by $p(x) = \\frac{1}{x!} \\lambda^x e^{-\\lambda}$.\n",
        "* **Beta, Dirichlet, Gamma, and Wishart Distributions** They are what statisticians call *conjugate* to the Binomial, Multinomial, Poisson and Gaussian respectively. Without going into detail, these distributions are often used as priors for coefficients of the latter set of distributions, e.g. a Beta distribution as a prior for modeling the probability for binomial outcomes.\n",
        "\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "So far, we covered probabilities, independence, conditional independence, and how to use this to draw some basic conclusions. We also introduced some fundamental probability distributions and demonstrated how to sample from them using PyTorch. This is already a powerful bit of knowledge, and by itself a sufficient set of tools for developing some classic machine learning models. In the next section, we will see how to operationalize this knowlege to build your first machine learning model: the Naive Bayes classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6vunisf5cx0",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Implement a function that returns the binomial distribution, which describes the number of times we toss a coin (n), each toss is for a fair unbiased coin (p)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHKA6l4R5a04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binomial(n, p):\n",
        "  ## write your code here\n",
        "  \n",
        "  ## end of function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oicPLbufEuFF",
        "colab_type": "text"
      },
      "source": [
        "# Automatic Differentiation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOvXbm_mEuFG",
        "colab_type": "text"
      },
      "source": [
        "The autograd package expedites this work by automatically calculating derivatives. And while many other libraries require that we compile a symbolic graph to take automatic derivatives, `autograd` allows us to take derivatives while writing  ordinary imperative code. Every time we pass data through our model, `autograd` builds a graph on the fly, tracking which data combined through which operations to produce the output. This graph enables `autograd` to subsequently backpropagate gradients on command. Here *backpropagate* simply means to trace through the compute graph, filling in the partial derivatives with respect to each parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdwc6qqKEuFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNlHx7sgEuFJ",
        "colab_type": "text"
      },
      "source": [
        "## A Simple Example\n",
        "\n",
        "As a toy example, say that we are interested in differentiating the mapping $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to the column vector $\\mathbf{x}$. To start, let's create the variable `x` and assign it an initial value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSGgVQikEuFJ",
        "colab_type": "code",
        "outputId": "2667be20-81ff-40d5-abfa-a896b25f3b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x = Variable(torch.arange(4, dtype=torch.float32).reshape((4, 1)), requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [3.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P4M2GIdEuFM",
        "colab_type": "text"
      },
      "source": [
        "Once we compute the gradient of ``y`` with respect to ``x``, we will need a place to store it. We can tell a tensor that we plan to store a gradient by the ``requires_grad=True`` keyword."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQCkgb4yEuFO",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to compute ``y`` and PyTorch will generate a computation graph on the fly. Autograd is reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
        "\n",
        "Note that building the computation graph requires a nontrivial amount of computation. So PyTorch will *only* build the graph when explicitly told to do so. For a tensor to be “recordable”, it must be wrapped with torch.autograd.Variable. The Variable class provides almost the same API as Tensor, but augments it with the ability to interplay with torch.autograd.Function in order to be differentiated automatically. More precisely, a Variable records the history of operations on a Tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U4-oEK-EuFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = 2*torch.mm(x.t(),x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2cvCFonEuFS",
        "colab_type": "text"
      },
      "source": [
        "Since the shape of `x` is (4, 1), `y` is a scalar. Next, we can automatically find the gradient by calling the `backward` function. It should be noted that if `y` is not a scalar, PyTorch will first sum the elements in `y` to get the new variable by default, and then find the gradient of the variable with respect to `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoGQ_KpiEuFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIiCORTBEuFX",
        "colab_type": "text"
      },
      "source": [
        "Since every Variable except for inputs is the result of an operation, each Variable has an associated grad_fn, which is the torch.autograd.Function that is used to compute the backward step. For inputs it is None:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbvv7Ip5EuFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"x.grad:\", x.grad)\n",
        "print(\"x.grad_fn:\", x.grad_fn)\n",
        "print(\"y.grad_fn:\", y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLyrcB-ZEuFa",
        "colab_type": "text"
      },
      "source": [
        "The gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$. Now let's verify that the gradient produced is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdfmiy_qEuFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((x.grad - 4*x).norm().item() == 0)\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW0H2NFuEuFd",
        "colab_type": "text"
      },
      "source": [
        "## Training Mode and Evaluation Mode\n",
        "\n",
        "`Model` will change the running mode to the evaluation mode on calling `model.eval()` or to the training mode on calling `model.train()`.\n",
        "\n",
        "In some cases, the same model behaves differently in the training and prediction modes (e.g. when using neural techniques such as dropout and batch normalization). In other cases, some models may store more auxiliary variables to make computing gradients easier. We will cover these differences in detail in later chapters. For now, you do not need to worry about them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfytDjnlEuFe",
        "colab_type": "text"
      },
      "source": [
        "## Computing the Gradient of Python Control Flow\n",
        "\n",
        "One benefit of using automatic differentiation is that even if the computational graph of the function contains Python's control flow (such as conditional and loop control), we may still be able to find the gradient of a variable. Consider the following program:  It should be emphasized that the number of iterations of the loop (while loop) and the execution of the conditional judgment (if statement) depend on the value of the input `b`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOxdaepmEuFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm().item() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum().item() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORHuaGBpEuFf",
        "colab_type": "text"
      },
      "source": [
        "Note that the number of iterations of the while loop and the execution of the conditional statement (if then else) depend on the value of `a`. To compute gradients, we need to `record` the calculation, and then call the `backward` function to calculate the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4AbD1hDEuFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.randn(size=(1,))\n",
        "a.requires_grad=True\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZGxru8fEuFh",
        "colab_type": "text"
      },
      "source": [
        "Let's analyze the `f` function defined above. As you can see, it is piecewise linear in its input `a`. In other words, for any `a` there exists some constant such that for a given range `f(a) = g * a`. Consequently `d / a` allows us to verify that the gradient is correct:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwjOESNqEuFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a.grad == (d / a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRA6eOZREuFm",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* PyTorch provides an `autograd` package to automate the derivation process.\n",
        "* PyTorch's `autograd` package can be used to derive general imperative programs.\n",
        "* The running modes of PyTorch include the training mode and the evaluation mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00y_Xys5EuFn",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes Classification\n",
        "\n",
        "Before we worry about complex optimization algorithms or GPUs, we can already deploy our first classifier, relying only on simple statistical estimators and our understanding of conditional independence. Learning is all about making assumptions. If we want to classify a new data point that we've never seen before we have to make some assumptions about which data points are *similar* to each other.\n",
        "\n",
        "One popular (and remarkably simple) algorithm is the Naive Bayes Classifier. Note that one natural way to express the classification task is via the probabilistic question: *what is the most likely label given the features?*. Formally, we wish to output the prediction $\\hat{y}$ given by the expression:\n",
        "\n",
        "$$\\hat{y} = \\text{argmax}_y \\> p(y | \\mathbf{x})$$\n",
        "\n",
        "Unfortunately, this requires that we estimate $p(y | \\mathbf{x})$ for every value of $\\mathbf{x} = x_1, ..., x_d$. Imagine that each feature could take one of $2$ values. For example, the feature $x_1 = 1$ might signify that the word apple appears in a given document and $x_1 = 0$ would signify that it does not. If we had $30$ such binary features, that would mean that we need to be prepared to classify any of $2^{30}$ (over 1 billion!) possible values of the input vector $\\mathbf{x}$.\n",
        "\n",
        "Moreover, where is the learning? If we need to see every single possible example in order to predict the corresponding label then we're not really learning a pattern but just memorizing the dataset. Fortunately, by making some assumptions about conditional independence, we can introduce some inductive bias and build a model capable of generalizing from a comparatively modest selection of training examples.\n",
        "\n",
        "To begin, let's use Bayes Theorem, to express the classifier as\n",
        "\n",
        "$$\\hat{y} = \\text{argmax}_y \\> \\frac{p( \\mathbf{x} | y) p(y)}{p(\\mathbf{x})}$$\n",
        "\n",
        "Note that the denominator is the normalizing term $p(\\mathbf{x})$ which does not depend on the value of the label $y$. As a result, we only need to worry about comparing the numerator across different values of $y$. Even if calculating the demoninator turned out to be intractable, we could get away with ignoring it, so long as we could evaluate the numerator. Fortunately, however, even if we wanted to recover the normalizing constant, we could, since we know that $\\sum_y p(y | \\mathbf{x}) = 1$, hence we can always recover the normalization term.\n",
        "Now, using the chain rule of probability, we can express the term $p( \\mathbf{x} | y)$ as\n",
        "\n",
        "$$p(x_1 |y) \\cdot p(x_2 | x_1, y) \\cdot ... \\cdot p( x_d | x_1, ..., x_{d-1} y)$$\n",
        "\n",
        "By itself, this expression doesn't get us any further. We still must estimate roughly $2^d$ parameters. However, if we assume that ***the features are conditionally independent of each other, given the label***, then suddenly we're in much better shape, as this term simplifies to $\\prod_i p(x_i | y)$, giving us the predictor\n",
        "\n",
        "$$ \\hat{y} = \\text{argmax}_y \\> = \\prod_i p(x_i | y) p(y)$$\n",
        "\n",
        "Estimating each term in $\\prod_i p(x_i | y)$ amounts to estimating just one parameter. So our assumption of conditional independence has taken the complexity of our model (in terms of the number of parameters) from an exponential dependence on the number of features to a linear dependence. Moreover, we can now make predictions for examples that we've never seen before, because we just need to estimate the terms $p(x_i | y)$, which can be estimated based on a number of different documents.\n",
        "\n",
        "Let's take a closer look at the key assumption that the attributes are all independent of each other, given the labels, i.e., $p(\\mathbf{x} | y) = \\prod_i p(x_i | y)$. Consider classifying emails into spam and ham. It's fair to say that the occurrence of the words `Nigeria`, `prince`, `money`, `rich` are all likely indicators that the e-mail might be spam, whereas `theorem`, `network`, `Bayes` or `statistics` are good indicators that the exchange is less likely to be part of an orchestrated attempt to wheedle out your bank account numbers. Thus, we could model the probability of occurrence for each of these words, given the respective class and then use it to score the likelihood of a text. In fact, for a long time this *is* preciely how many so-called [Bayesian spam filters](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering) worked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTmuhTbWEuFn",
        "colab_type": "text"
      },
      "source": [
        "## Optical Character Recognition\n",
        "\n",
        "Since images are much easier to deal with, we will illustrate the workings of a Naive Bayes classifier for distinguishing digits on the MNIST dataset. The problem is that we don't actually know $p(y)$ and $p(x_i | y)$. So we need to *estimate* it given some training data first. This is what is called *training* the model. Estimating $p(y)$ is not too hard. Since we are only dealing with 10 classes, this is pretty easy - simply count the number of occurrences $n_y$ for each of the digits and divide it by the total amount of data $n$. For instance, if digit 8 occurs $n_8 = 5,800$ times and we have a total of $n = 60,000$ images, the probability estimate is $p(y=8) = 0.0967$.\n",
        "\n",
        "Now on to slightly more difficult things—$p(x_i | y)$. Since we picked black and white images, $p(x_i | y)$ denotes the probability that pixel $i$ is switched on for class $y$. Just like before we can go and count the number of times $n_{iy}$ such that an event occurs and divide it by the total number of occurrences of y, i.e. $n_y$. But there's something slightly troubling: certain pixels may never be black (e.g. for very well cropped images the corner pixels might always be white). A convenient way for statisticians to deal with this problem is to add pseudo counts to all occurrences. Hence, rather than $n_{iy}$ we use $n_{iy}+1$ and instead of $n_y$ we use $n_{y} + 1$. This is also called [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxqoHh9bEuFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')\n",
        "import torch\n",
        "from torch import tensor\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "data_transform = transforms.Compose([transforms.Grayscale(), transforms.ToTensor(), transforms.Normalize(mean=[0],std=[1])])\n",
        "\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=data_transform)\n",
        "mnist_test  = datasets.MNIST(root='./data', train=False, download=True, transform=data_transform)\n",
        "\n",
        "# Initialize the counters\n",
        "xcount = torch.ones((784,10), dtype=torch.float32)\n",
        "ycount = torch.ones((10), dtype=torch.float32)\n",
        "\n",
        "for data, label in mnist_train:\n",
        "    y = int(label)\n",
        "    ycount[y] += 1\n",
        "    xcount[:,y] += data.reshape((784))\n",
        "\n",
        "# using broadcast again for division\n",
        "py = ycount / ycount.sum()\n",
        "px = (xcount / ycount.reshape(1,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgtUTRPuEuFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data, label in mnist_train:\n",
        "    y = int(label)\n",
        "    ycount[y] += 1\n",
        "    xcount[:,y] += data.reshape((784))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WssBkEfEuFx",
        "colab_type": "text"
      },
      "source": [
        "Now that we computed per-pixel counts of occurrence for all pixels, it's time to see how our model behaves. Time to plot it. This is where it is so much more convenient to work with images. Visualizing 28x28x10 probabilities (for each pixel for each class) would typically be an exercise in futility. However, by plotting them as images we get a quick overview. The astute reader probably noticed by now that these are some mean looking digits ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDoS3S7rEuFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, figarr = plt.subplots(1, 10, figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    figarr[i].imshow(xcount[:, i].reshape((28, 28)).numpy(), cmap='hot')\n",
        "    figarr[i].axes.get_xaxis().set_visible(False)\n",
        "    figarr[i].axes.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n",
        "print('Class probabilities', py)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtkoEdw3EuFz",
        "colab_type": "text"
      },
      "source": [
        "Now we can compute the likelihoods of an image, given the model. This is statistician speak for $p(x | y)$, i.e. how likely it is to see a particular image under certain conditions (such as the label). Our Naive Bayes model which assumed that all pixels are independent tells us that\n",
        "\n",
        "$$p(\\mathbf{x} | y) = \\prod_{i} p(x_i | y)$$\n",
        "\n",
        "Using Bayes' rule, we can thus compute $p(y | \\mathbf{x})$ via\n",
        "\n",
        "$$p(y | \\mathbf{x}) = \\frac{p(\\mathbf{x} | y) p(y)}{\\sum_{y'} p(\\mathbf{x} | y')}$$\n",
        "\n",
        "Let's try this ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM6NE0BbEuF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the first test item\n",
        "data, label = mnist_test[0]\n",
        "data = data.reshape((784,1))\n",
        "\n",
        "# Compute the per pixel conditional probabilities\n",
        "xprob = (px * data + (1-px) * (1-data))\n",
        "# Take the product\n",
        "xprob = xprob.prod(0) * py\n",
        "print('Unnormalized Probabilities', xprob)\n",
        "# Normalize\n",
        "xprob = xprob / xprob.sum()\n",
        "print('Normalized Probabilities', xprob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qlQ5PAvEuF4",
        "colab_type": "text"
      },
      "source": [
        "This went horribly wrong! To find out why, let's look at the per pixel probabilities. They're typically numbers between $0.001$ and $1$. We are multiplying $784$ of them. At this point it is worth mentioning that we are calculating these numbers on a computer, hence with a fixed range for the exponent. What happens is that we experience *numerical underflow*, i.e. multiplying all the small numbers leads to something even smaller until it is rounded down to zero. At that point we get division by zero with `nan` as a result.\n",
        "\n",
        "To fix this we use the fact that $\\log a b = \\log a + \\log b$, i.e. we switch to summing logarithms. This will get us unnormalized probabilities in log-space. To normalize terms we use the fact that\n",
        "\n",
        "$$\\frac{\\exp(a)}{\\exp(a) + \\exp(b)} = \\frac{\\exp(a + c)}{\\exp(a + c) + \\exp(b + c)}$$\n",
        "\n",
        "In particular, we can pick $c = -\\max(a,b)$, which ensures that at least one of the terms in the denominator is $1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i52kOrlzEuF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logpx = torch.log(px)\n",
        "logpxneg = torch.log(1-px)\n",
        "logpy = torch.log(py)\n",
        "\n",
        "def bayespost(data):\n",
        "    # We need to incorporate the prior probability p(y) since p(y|x) is\n",
        "    # proportional to p(x|y) p(y)\n",
        "    logpost = logpy.clone()\n",
        "    logpost += (logpx * data + logpxneg * (1-data)).sum(0)\n",
        "    # Normalize to prevent overflow or underflow by subtracting the largest\n",
        "    # value\n",
        "    logpost -= torch.max(logpost)\n",
        "    # Compute the softmax using logpx\n",
        "    post = torch.exp(logpost).numpy()\n",
        "    post /= np.sum(post)\n",
        "    return post\n",
        "\n",
        "fig, figarr = plt.subplots(2, 10, figsize=(10, 3))\n",
        "\n",
        "# Show 10 images\n",
        "ctr = 0\n",
        "for data, label in mnist_test:\n",
        "    x = data.reshape((784,1))\n",
        "    y = int(label)\n",
        "\n",
        "    post = bayespost(x)\n",
        "\n",
        "    # Bar chart and image of digit\n",
        "    figarr[1, ctr].bar(range(10), post)\n",
        "    figarr[1, ctr].axes.get_yaxis().set_visible(False)\n",
        "    figarr[0, ctr].imshow(x.reshape((28, 28)).numpy(), cmap='hot')\n",
        "    figarr[0, ctr].axes.get_xaxis().set_visible(False)\n",
        "    figarr[0, ctr].axes.get_yaxis().set_visible(False)\n",
        "    ctr += 1\n",
        "\n",
        "    if ctr == 10:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQkliTEJEuF6",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** As we can see, this classifier works pretty well in many cases. However, the second last digit shows that it can be both incompetent and overly confident of its incorrect estimates. That is, even if it is horribly wrong, it generates probabilities close to 1 or 0. Not a classifier we should use very much nowadays any longer. To see how well it performs overall, let's compute the overall accuracy of the classifier. Write a function that computes the accuracy of the above naive bayes classifier on MNIST test set. *HINT: utilize the `bayespost` function from above.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMP21EMuEuF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_NB(mnist_test):\n",
        "  acc = 0.0\n",
        "  ## write your code here\n",
        "\n",
        "  ## end of function\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d10-FGtTEuF8",
        "colab_type": "text"
      },
      "source": [
        "Modern deep networks achieve error rates of less than 0.01. While Naive Bayes classifiers used to be popular in the 80s and 90s, e.g. for spam filtering, their heydays are over. The poor performance is due to the incorrect statistical assumptions that we made in our model: we assumed that each and every pixel are *independently* generated, depending only on the label. This is clearly not how humans write digits, and this wrong assumption led to the downfall of our overly naive (Bayes) classifier. Time to start building Deep Networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjbCt-ECEuF8",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Naive Bayes is an easy to use classifier that uses the assumption\n",
        "  $p(\\mathbf{x} | y) = \\prod_i p(x_i | y)$.\n",
        "* The classifier is easy to train but its estimates can be very wrong.\n",
        "* To address overly confident and nonsensical estimates, the\n",
        "  probabilities $p(x_i|y)$ are smoothed, e.g. by Laplace\n",
        "  smoothing. That is, we add a constant to all counts.\n",
        "* Naive Bayes classifiers don't exploit any correlations between\n",
        "  observations."
      ]
    }
  ]
}